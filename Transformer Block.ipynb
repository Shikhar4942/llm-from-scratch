{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19984fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_2_CONFIGURATION_124M={\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\":1024,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_heads\":12,\n",
    "    \"n_layers\":12,\n",
    "    \"dropout\":0.1,\n",
    "    \"qkv_bias\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af7e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298db15d",
   "metadata": {},
   "source": [
    "# layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1107278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4c9901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
       "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_ex=torch.randn(2,5)\n",
    "layer=nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out=layer(batch_ex)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e278ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=out.mean(dim=-1,keepdim=True)\n",
    "var=out.var(dim=-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b4e96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1324],\n",
       "        [0.2170]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7756d43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0231],\n",
       "        [0.0398]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a72d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_out=(out-mean)/(var**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64fbf438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
       "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981756aa",
   "metadata": {},
   "source": [
    "# layer norm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aee2ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2107771",
   "metadata": {},
   "source": [
    "self.scale and shift are trainable parameters.\n",
    "Why Scale and Shift are Necessary:\n",
    "Recovering Representation:\n",
    "\n",
    "Normalizing the inputs to zero mean and unit variance can make the training more stable, but it may erase important features in the raw input data.\n",
    "The scale and shift parameters provide the network the ability to restore this lost information if needed.\n",
    "Learning Flexibility:\n",
    "\n",
    "By learning the optimal scale and shift values during training, the network can adapt normalization to suit the specific requirements of the task.\n",
    "For example, certain activation functions (like ReLU or Tanh) may benefit from non-zero mean or non-unit variance.\n",
    "Non-Linearity Preservation:\n",
    "\n",
    "Normalization without scale and shift could restrict the representational capacity, especially when layers are stacked. With scale and shift, the network can preserve or enhance non-linear transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a00281c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ln=LayerNorm(emb_dim=5)\n",
    "out_ln=ln(batch_ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44209327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4945,  0.9564, -0.0200,  0.2375, -1.6685],\n",
       "        [ 0.8127, -1.2313, -0.8554,  1.0110,  0.2630]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9818eca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4901e-08,  2.3842e-08], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ln.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19f5d0",
   "metadata": {},
   "source": [
    "# GeLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0adcfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Precompute constant √(2/π) as a buffer for efficiency\n",
    "        self.sqrt_2_pi = math.sqrt(2.0 / math.pi)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure dtype and device compatibility\n",
    "        sqrt_2_pi = torch.tensor(self.sqrt_2_pi, dtype=x.dtype, device=x.device)\n",
    "        return 0.5 * x * (1 + torch.tanh(sqrt_2_pi * (x + 0.044715 * torch.pow(x, 3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "442013c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
    "                                 GELU(),\n",
    "                                 nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "                                 )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162ab540",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn=FeedForward(GPT_2_CONFIGURATION_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3856c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(2,3,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6955d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3731, -0.2161,  0.1972,  ..., -0.2462,  0.0535,  0.2413],\n",
       "         [ 0.0069,  0.0609,  0.3952,  ...,  0.1626, -0.0415, -0.1237],\n",
       "         [ 0.1569, -0.1565, -0.0789,  ..., -0.3007,  0.2389, -0.1702]],\n",
       "\n",
       "        [[ 0.2887,  0.0783,  0.1038,  ..., -0.2605, -0.0504, -0.2268],\n",
       "         [-0.0889,  0.2274,  0.0563,  ..., -0.2062,  0.0148, -0.2420],\n",
       "         [ 0.2520, -0.0005, -0.2848,  ..., -0.0739, -0.0354,  0.0410]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3416a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c16dfa",
   "metadata": {},
   "source": [
    "# example usage of skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b604f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self,layer,use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut=use_shortcut\n",
    "        self.layers=nn.ModuleList([\n",
    "        nn.Sequential(nn.Linear(layer[0],layer[1]),GELU()),\n",
    "        nn.Sequential(nn.Linear(layer[1],layer[2]),GELU()),\n",
    "        nn.Sequential(nn.Linear(layer[2],layer[3]),GELU()),\n",
    "        nn.Sequential(nn.Linear(layer[3],layer[4]),GELU()),\n",
    "        nn.Sequential(nn.Linear(layer[4],layer[5]),GELU())\n",
    "        ])\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            layer_output=layer(x)\n",
    "            if self.use_shortcut and x.shape==layer_output.shape:\n",
    "                x=x+layer_output\n",
    "            else:\n",
    "                x=layer_output\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "960a0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size=[3,3,3,3,3,1]\n",
    "sample_ip=torch.tensor([[1.,0.,-1.]])\n",
    "\n",
    "op_wo_shortcut=DNN(layer_size,use_shortcut=False)\n",
    "op=op_wo_shortcut(sample_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22663047",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_w_shortcut=DNN(layer_size,use_shortcut=True)\n",
    "op_=op_w_shortcut(sample_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b15ee5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad(model, x):\n",
    "    # Perform forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    # Define the target tensor\n",
    "    target = torch.zeros_like(output)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(output, target)\n",
    "    \n",
    "    # Clear previous gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print gradient mean for weights\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.grad is not None:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b16545ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 4.251542122801766e-06\n",
      "layers.1.0.weight has gradient mean of 3.5761568142334e-05\n",
      "layers.2.0.weight has gradient mean of 9.792059427127242e-05\n",
      "layers.3.0.weight has gradient mean of 0.00019723761943168938\n",
      "layers.4.0.weight has gradient mean of 0.0018265097169205546\n"
     ]
    }
   ],
   "source": [
    "print_grad(op_wo_shortcut,sample_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3103de87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.5702555775642395\n",
      "layers.1.0.weight has gradient mean of 0.7371833324432373\n",
      "layers.2.0.weight has gradient mean of 0.8664534687995911\n",
      "layers.3.0.weight has gradient mean of 0.6517516374588013\n",
      "layers.4.0.weight has gradient mean of 4.082286834716797\n"
     ]
    }
   ],
   "source": [
    "print_grad(op_w_shortcut,sample_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05cf1a2",
   "metadata": {},
   "source": [
    "# whole transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2210e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Precompute constant √(2/π) as a buffer for efficiency\n",
    "        self.sqrt_2_pi = math.sqrt(2.0 / math.pi)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure dtype and device compatibility\n",
    "        sqrt_2_pi = torch.tensor(self.sqrt_2_pi, dtype=x.dtype, device=x.device)\n",
    "        return 0.5 * x * (1 + torch.tanh(sqrt_2_pi * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
    "                                 GELU(),\n",
    "                                 nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "                                 )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c159b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_Attention_V2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,num_heads,dropout,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        self.d_out=d_out\n",
    "        self.num_heads=num_heads\n",
    "        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_keys=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_values=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.head_dim=d_out//num_heads\n",
    "        self.out_proj=nn.Linear(d_out,d_out)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in=x.shape\n",
    "        keys=self.W_keys(x)\n",
    "        queries=self.W_query(x)\n",
    "        values=self.W_values(x)\n",
    "        #now change dimensions\n",
    "        \n",
    "        keys=keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values=values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        #group as num_heads\n",
    "        keys=keys.transpose(1,2)\n",
    "        queries=queries.transpose(1,2)\n",
    "        values=values.transpose(1,2)\n",
    "        \n",
    "        attn_scores=queries @ keys.transpose(2,3)\n",
    "        \n",
    "        mask_bool=self.mask.bool()[:num_tokens, :num_tokens]  #if num_tokens is less than specified context length\n",
    "        \n",
    "        attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
    "        \n",
    "        attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "        \n",
    "        context_vectors=(attn_weights @ values).transpose(1,2)\n",
    "        context_vectors=context_vectors.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vectors=self.out_proj(context_vectors)\n",
    "        \n",
    "        return context_vectors\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38674033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att=Multihead_Attention_V2(d_in=cfg[\"emb_dim\"],\n",
    "                                       d_out=cfg[\"emb_dim\"],\n",
    "                                       context_length=cfg[\"context_length\"],\n",
    "                                       num_heads=cfg[\"n_heads\"],\n",
    "                                       dropout=cfg[\"dropout\"],\n",
    "                                       qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff=FeedForward(cfg)\n",
    "        self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut=nn.Dropout(cfg[\"dropout\"])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        shortcut=x;\n",
    "        x=self.norm1(x)\n",
    "        x=self.att(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "        \n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ba6c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "x=torch.rand(2,4,768)\n",
    "block=TransformerBlock(GPT_2_CONFIGURATION_124M)\n",
    "output=block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a00ad252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1649,  0.4003, -0.0746,  ...,  1.2644,  0.3327,  0.7242],\n",
       "         [ 0.0295,  0.0499,  0.2529,  ...,  0.4699,  0.1284,  0.9746],\n",
       "         [ 0.5534,  0.5785, -0.0309,  ...,  1.1541,  0.3949,  0.7598],\n",
       "         [ 0.1631,  0.7129,  0.7272,  ...,  0.3312,  0.5731,  0.9255]],\n",
       "\n",
       "        [[ 0.1788,  1.1680,  0.5809,  ...,  0.1828,  0.0076, -0.5598],\n",
       "         [-0.2919,  0.6317,  0.2002,  ...,  0.3218,  0.4671, -0.0381],\n",
       "         [ 0.9273,  0.4202,  0.3183,  ...,  0.3771,  0.7189, -0.1203],\n",
       "         [ 0.6033,  0.5767,  0.3411,  ...,  1.3796,  1.2681,  0.3915]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b264dce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2961, 0.5166, 0.2517,  ..., 0.9541, 0.8567, 0.4604],\n",
       "         [0.2238, 0.3047, 0.3019,  ..., 0.5465, 0.4532, 0.7598],\n",
       "         [0.6945, 0.2478, 0.4111,  ..., 0.8838, 0.4898, 0.5963],\n",
       "         [0.0890, 0.7804, 0.9223,  ..., 0.4507, 0.6357, 0.5833]],\n",
       "\n",
       "        [[0.5716, 0.9297, 0.3396,  ..., 0.0477, 0.4564, 0.2797],\n",
       "         [0.0936, 0.2211, 0.3806,  ..., 0.3948, 0.4545, 0.4536],\n",
       "         [0.6788, 0.1741, 0.2084,  ..., 0.5557, 0.5930, 0.0959],\n",
       "         [0.3894, 0.4083, 0.0662,  ..., 0.9861, 0.9341, 0.1319]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea85ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
